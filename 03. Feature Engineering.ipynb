{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create features from the raw text so we can train the machine learning models. The steps followed are:\n",
    "\n",
    "1. **Text Cleaning and Preparation**: cleaning of special characters, downcasing, punctuation signs. possessive pronouns and stop words removal and lemmatization. \n",
    "2. **Label coding**: creation of a dictionary to map each category to a code.\n",
    "3. **Train-test split**: to test the models on unseen data.\n",
    "4. **Text representation**: use of TF-IDF scores to represent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we'll load the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_df = \"Contract_dataset.pickle\"\n",
    "\n",
    "with open(path_df, 'rb') as data:\n",
    "    df = pickle.load(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tender Title</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tender No and date</th>\n",
       "      <th>Plant/Unit</th>\n",
       "      <th>Tender issue date and time</th>\n",
       "      <th>Bid Submission Closing date and Time</th>\n",
       "      <th>Label_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Procurement of water treatment chemicals for 4...</td>\n",
       "      <td>raw_materials</td>\n",
       "      <td>000000001738 Dt. 17/01/2020</td>\n",
       "      <td>IISCO Steel Plant</td>\n",
       "      <td>Feb 18 2020  5:00:00:000PM</td>\n",
       "      <td>Mar 25 2020 12:00:00:000PM</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SPARES FOR ROD MILL LINER</td>\n",
       "      <td>hardware</td>\n",
       "      <td>003/215/1902000911/500006763/01/00</td>\n",
       "      <td>Rourkela Steel Plant</td>\n",
       "      <td>Jan  8 2020  8:00:00:000PM</td>\n",
       "      <td>May 21 2020  4:00:00:000PM</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPARES FOR OVEN INTERLOCKING SYSTEM OF COB-1,3...</td>\n",
       "      <td>hardware</td>\n",
       "      <td>003/340/1902000406/01/00/500006867 dated 21.01...</td>\n",
       "      <td>Rourkela Steel Plant</td>\n",
       "      <td>Feb 26 2020  7:00:00:000PM</td>\n",
       "      <td>Mar 26 2020  4:00:00:000PM</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORIFICE  FLOW METER FOR RAW WATER RISING MAIN</td>\n",
       "      <td>hardware</td>\n",
       "      <td>003/530/1902002486/01/00/500006927 DTD.20.02.2020</td>\n",
       "      <td>Rourkela Steel Plant</td>\n",
       "      <td>Feb 20 2020  8:00:00:000PM</td>\n",
       "      <td>Apr 20 2020  4:00:00:000PM</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FIRE FIGHTG ENSEMBLE SET (JACKET,TROUSER,BOOT,...</td>\n",
       "      <td>none</td>\n",
       "      <td>004/007/1848000076/02/00/500006945  DATED:05.0...</td>\n",
       "      <td>Rourkela Steel Plant</td>\n",
       "      <td>Apr 29 2020  4:00:00:000PM</td>\n",
       "      <td>May 20 2020  4:00:00:000PM</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Tender Title          Label  \\\n",
       "0  Procurement of water treatment chemicals for 4...  raw_materials   \n",
       "1                          SPARES FOR ROD MILL LINER       hardware   \n",
       "2  SPARES FOR OVEN INTERLOCKING SYSTEM OF COB-1,3...       hardware   \n",
       "3      ORIFICE  FLOW METER FOR RAW WATER RISING MAIN       hardware   \n",
       "4  FIRE FIGHTG ENSEMBLE SET (JACKET,TROUSER,BOOT,...           none   \n",
       "\n",
       "                                  Tender No and date            Plant/Unit  \\\n",
       "0                        000000001738 Dt. 17/01/2020     IISCO Steel Plant   \n",
       "1                 003/215/1902000911/500006763/01/00  Rourkela Steel Plant   \n",
       "2  003/340/1902000406/01/00/500006867 dated 21.01...  Rourkela Steel Plant   \n",
       "3  003/530/1902002486/01/00/500006927 DTD.20.02.2020  Rourkela Steel Plant   \n",
       "4  004/007/1848000076/02/00/500006945  DATED:05.0...  Rourkela Steel Plant   \n",
       "\n",
       "   Tender issue date and time Bid Submission Closing date and Time  Label_Code  \n",
       "0  Feb 18 2020  5:00:00:000PM           Mar 25 2020 12:00:00:000PM           5  \n",
       "1  Jan  8 2020  8:00:00:000PM           May 21 2020  4:00:00:000PM           2  \n",
       "2  Feb 26 2020  7:00:00:000PM           Mar 26 2020  4:00:00:000PM           2  \n",
       "3  Feb 20 2020  8:00:00:000PM           Apr 20 2020  4:00:00:000PM           2  \n",
       "4  Apr 29 2020  4:00:00:000PM           May 20 2020  4:00:00:000PM           4  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize one sample news content:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Text cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Special character cleaning\n",
    "\n",
    "We can see the following special characters:\n",
    "\n",
    "* ``\\r``\n",
    "* ``\\n``\n",
    "* ``\\`` before possessive pronouns (`government's = government\\'s`)\n",
    "* ``\\`` before possessive pronouns 2 (`Yukos'` = `Yukos\\'`)\n",
    "* ``\"`` when quoting text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \\r and \\n\n",
    "df['Content_Parsed_1'] = df['Tender Title'].str.replace(\"\\r\", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"\\n\", \" \")\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace(\"    \", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding 3rd and 4th bullet, although it seems there is a special character, it won't affect us since it is not a *real* character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Mr Greenspan\\'s\"\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" when quoting text\n",
    "df['Content_Parsed_1'] = df['Content_Parsed_1'].str.replace('\"', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Upcase/downcase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll downcase the texts because we want, for example, `Football` and `football` to be the same word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing the text\n",
    "df['Content_Parsed_2'] = df['Content_Parsed_1'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Punctuation signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Punctuation signs won't have any predicting power, so we'll just get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_signs = list(\"?:!.,;\")\n",
    "df['Content_Parsed_3'] = df['Content_Parsed_2']\n",
    "\n",
    "for punct_sign in punctuation_signs:\n",
    "    df['Content_Parsed_3'] = df['Content_Parsed_3'].str.replace(punct_sign, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing this we are messing up with some numbers, but it's no problem since we aren't expecting any predicting power from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Possessive pronouns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also remove possessive pronoun terminations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_4'] = df['Content_Parsed_3'].str.replace(\"'s\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      procurement of water treatment chemicals for 4...\n",
       "1                              spares for rod mill liner\n",
       "2      spares for oven interlocking system of cob-135...\n",
       "3          orifice  flow meter for raw water rising main\n",
       "4      fire fightg ensemble set (jackettrouserbooth/g...\n",
       "                             ...                        \n",
       "734    major modification/renovation of cisf colony a...\n",
       "735    face lifting of single storied buildings at cd...\n",
       "736    face lifting of two storied buildings at cd to...\n",
       "737    building maintenance for sail house guest hous...\n",
       "738                                 allotment of canteen\n",
       "Name: Content_Parsed_4, Length: 734, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Content_Parsed_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Stemming and Lemmatization\n",
    "\n",
    "Since stemming can produce output words that don't exist, we'll only use a lemmatization process at this moment. Lemmatization takes into consideration the morphological analysis of the words and returns words that do exist, so it will be more useful for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ayushi.Goel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ayushi.Goel\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading punkt and wordnet from NLTK\n",
    "nltk.download('punkt')\n",
    "print(\"------------------------------------------------------------\")\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the lemmatizer into an object\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to lemmatize, we have to iterate through every word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tender Title                            REVAMPING OF QUENCHING CAR WAGON AND REPAIR OF...\n",
       "Label                                                                    skilled_manpower\n",
       "Tender No and date                      NIT No : GM ( CC- W) / Coke Oven-Mech Mnt / EP...\n",
       "Plant/Unit                                                             Bhilai Steel Plant\n",
       "Tender issue date and time                                     Mar 12 2020 12:00:00:000PM\n",
       "Bid Submission Closing date and Time                           Mar 24 2020  2:00:00:000PM\n",
       "Label_Code                                                                              6\n",
       "Content_Parsed_1                        REVAMPING OF QUENCHING CAR WAGON AND REPAIR OF...\n",
       "Content_Parsed_2                        revamping of quenching car wagon and repair of...\n",
       "Content_Parsed_3                        revamping of quenching car wagon and repair of...\n",
       "Content_Parsed_4                        revamping of quenching car wagon and repair of...\n",
       "Name: 603, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = len(df)\n",
    "lemmatized_text_list = []\n",
    "\n",
    "for row in range(0, nrows):\n",
    "    #print (row)\n",
    "    # Create an empty list containing lemmatized words\n",
    "    lemmatized_list = []\n",
    "    \n",
    "    # Save the text and its words into an object\n",
    "    text = df.iloc[row]['Content_Parsed_4']\n",
    "    text_words = str(text).split(\" \")\n",
    "\n",
    "    # Iterate through every word to lemmatize\n",
    "    for word in text_words:\n",
    "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        \n",
    "    # Join the list\n",
    "    lemmatized_text = \" \".join(lemmatized_list)\n",
    "    \n",
    "    # Append to the list containing the texts\n",
    "    lemmatized_text_list.append(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_5'] = lemmatized_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although lemmatization doesn't work perfectly in all cases (as can be seen in the example below), it can be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ayushi.Goel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Downloading the stop words list\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stop words in english\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the stop words, we'll handle a regular expression only detecting whole words, as seen in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'StopWord eating a meal'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = \"me eating a meal\"\n",
    "word = \"me\"\n",
    "\n",
    "# The regular expression is:\n",
    "regex = r\"\\b\" + word + r\"\\b\"  # we need to build it like that to work properly\n",
    "\n",
    "re.sub(regex, \"StopWord\", example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now loop through all the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Content_Parsed_6'] = df['Content_Parsed_5']\n",
    "\n",
    "for stop_word in stop_words:\n",
    "\n",
    "    regex_stopword = r\"\\b\" + stop_word + r\"\\b\"\n",
    "    df['Content_Parsed_6'] = df['Content_Parsed_6'].str.replace(regex_stopword, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have some dobule/triple spaces between words because of the replacements. However, it's not a problem because we'll tokenize by the spaces later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we'll show an original news article and its modifications throughout the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OLFA FOR SALE OF COAL CHEMICALS'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[20]['Tender Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Special character cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OLFA FOR SALE OF COAL CHEMICALS'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[20]['Content_Parsed_1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Upcase/downcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olfa for sale of coal chemicals'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[20]['Content_Parsed_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Punctuation signs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olfa for sale of coal chemicals'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[20]['Content_Parsed_3']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Possessive pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olfa for sale of coal chemicals'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[20]['Content_Parsed_4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olfa for sale of coal chemicals'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[20]['Content_Parsed_5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'olfa  sale  coal chemicals'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[20]['Content_Parsed_6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can delete the intermediate columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tender Title</th>\n",
       "      <th>Label</th>\n",
       "      <th>Tender No and date</th>\n",
       "      <th>Plant/Unit</th>\n",
       "      <th>Tender issue date and time</th>\n",
       "      <th>Bid Submission Closing date and Time</th>\n",
       "      <th>Label_Code</th>\n",
       "      <th>Content_Parsed_1</th>\n",
       "      <th>Content_Parsed_2</th>\n",
       "      <th>Content_Parsed_3</th>\n",
       "      <th>Content_Parsed_4</th>\n",
       "      <th>Content_Parsed_5</th>\n",
       "      <th>Content_Parsed_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Procurement of water treatment chemicals for 4...</td>\n",
       "      <td>raw_materials</td>\n",
       "      <td>000000001738 Dt. 17/01/2020</td>\n",
       "      <td>IISCO Steel Plant</td>\n",
       "      <td>Feb 18 2020  5:00:00:000PM</td>\n",
       "      <td>Mar 25 2020 12:00:00:000PM</td>\n",
       "      <td>5</td>\n",
       "      <td>Procurement of water treatment chemicals for 4...</td>\n",
       "      <td>procurement of water treatment chemicals for 4...</td>\n",
       "      <td>procurement of water treatment chemicals for 4...</td>\n",
       "      <td>procurement of water treatment chemicals for 4...</td>\n",
       "      <td>procurement of water treatment chemicals for 4...</td>\n",
       "      <td>procurement  water treatment chemicals  4161 m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Tender Title          Label  \\\n",
       "0  Procurement of water treatment chemicals for 4...  raw_materials   \n",
       "\n",
       "            Tender No and date         Plant/Unit  Tender issue date and time  \\\n",
       "0  000000001738 Dt. 17/01/2020  IISCO Steel Plant  Feb 18 2020  5:00:00:000PM   \n",
       "\n",
       "  Bid Submission Closing date and Time  Label_Code  \\\n",
       "0           Mar 25 2020 12:00:00:000PM           5   \n",
       "\n",
       "                                    Content_Parsed_1  \\\n",
       "0  Procurement of water treatment chemicals for 4...   \n",
       "\n",
       "                                    Content_Parsed_2  \\\n",
       "0  procurement of water treatment chemicals for 4...   \n",
       "\n",
       "                                    Content_Parsed_3  \\\n",
       "0  procurement of water treatment chemicals for 4...   \n",
       "\n",
       "                                    Content_Parsed_4  \\\n",
       "0  procurement of water treatment chemicals for 4...   \n",
       "\n",
       "                                    Content_Parsed_5  \\\n",
       "0  procurement of water treatment chemicals for 4...   \n",
       "\n",
       "                                    Content_Parsed_6  \n",
       "0  procurement  water treatment chemicals  4161 m...  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_columns = [\"Label\",  \"Tender Title\", \"Content_Parsed_6\", \"Label_Code\"]\n",
    "df = df[list_columns]\n",
    "\n",
    "df = df.rename(columns={'Content_Parsed_6': 'Content_Parsed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tender Title</th>\n",
       "      <th>Content_Parsed</th>\n",
       "      <th>Label_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw_materials</td>\n",
       "      <td>Procurement of water treatment chemicals for 4...</td>\n",
       "      <td>procurement  water treatment chemicals  4161 m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hardware</td>\n",
       "      <td>SPARES FOR ROD MILL LINER</td>\n",
       "      <td>spar  rod mill liner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hardware</td>\n",
       "      <td>SPARES FOR OVEN INTERLOCKING SYSTEM OF COB-1,3...</td>\n",
       "      <td>spar  oven interlock system  cob-135  6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hardware</td>\n",
       "      <td>ORIFICE  FLOW METER FOR RAW WATER RISING MAIN</td>\n",
       "      <td>orifice  flow meter  raw water rise main</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>none</td>\n",
       "      <td>FIRE FIGHTG ENSEMBLE SET (JACKET,TROUSER,BOOT,...</td>\n",
       "      <td>fire fightg ensemble set (jackettrouserbooth/g...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Label                                       Tender Title  \\\n",
       "0  raw_materials  Procurement of water treatment chemicals for 4...   \n",
       "1       hardware                          SPARES FOR ROD MILL LINER   \n",
       "2       hardware  SPARES FOR OVEN INTERLOCKING SYSTEM OF COB-1,3...   \n",
       "3       hardware      ORIFICE  FLOW METER FOR RAW WATER RISING MAIN   \n",
       "4           none  FIRE FIGHTG ENSEMBLE SET (JACKET,TROUSER,BOOT,...   \n",
       "\n",
       "                                      Content_Parsed  Label_Code  \n",
       "0  procurement  water treatment chemicals  4161 m...           5  \n",
       "1                               spar  rod mill liner           2  \n",
       "2            spar  oven interlock system  cob-135  6           2  \n",
       "3           orifice  flow meter  raw water rise main           2  \n",
       "4  fire fightg ensemble set (jackettrouserbooth/g...           4  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:**\n",
    "\n",
    "We need to remember that our model will gather the latest news articles from different newspapers every time we want. For that reason, we not only need to take into account the peculiarities of the training set articles, but also possible ones that are present in the gathered news articles.\n",
    "\n",
    "For this reason, possible peculiarities have been studied in the *05. News Scraping* folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Label coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a dictionary with the label codification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skilled_manpower            246\n",
       "none                        160\n",
       "unskilled_manpower           94\n",
       "raw_materials                88\n",
       "hardware                     46\n",
       "vehicle/equipment_hiring     42\n",
       "machine                      35\n",
       "electronics                  23\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_codes = {\n",
    "#     'skilled_manpower': 1,\n",
    "#     'none': 2,\n",
    "#     'unskilled_manpower': 3,\n",
    "#     'raw_materials': 4,\n",
    "#     'hardware': 5,\n",
    "#     'vehicle/equipment_hiring': 6,\n",
    "#     'machine': 7,\n",
    "#     'electronics': 8\n",
    "    \n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Category mapping\n",
    "# df['Label_Code'] = df['Label']\n",
    "# df = df.replace({'Label_Code':category_codes})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    246\n",
       "4    160\n",
       "7     94\n",
       "5     88\n",
       "2     46\n",
       "8     42\n",
       "3     35\n",
       "1     23\n",
       "Name: Label_Code, dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Label_Code.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Tender Title</th>\n",
       "      <th>Content_Parsed</th>\n",
       "      <th>Label_Code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>raw_materials</td>\n",
       "      <td>Procurement of water treatment chemicals for 4...</td>\n",
       "      <td>procurement  water treatment chemicals  4161 m...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hardware</td>\n",
       "      <td>SPARES FOR ROD MILL LINER</td>\n",
       "      <td>spar  rod mill liner</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hardware</td>\n",
       "      <td>SPARES FOR OVEN INTERLOCKING SYSTEM OF COB-1,3...</td>\n",
       "      <td>spar  oven interlock system  cob-135  6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hardware</td>\n",
       "      <td>ORIFICE  FLOW METER FOR RAW WATER RISING MAIN</td>\n",
       "      <td>orifice  flow meter  raw water rise main</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>none</td>\n",
       "      <td>FIRE FIGHTG ENSEMBLE SET (JACKET,TROUSER,BOOT,...</td>\n",
       "      <td>fire fightg ensemble set (jackettrouserbooth/g...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Label                                       Tender Title  \\\n",
       "0  raw_materials  Procurement of water treatment chemicals for 4...   \n",
       "1       hardware                          SPARES FOR ROD MILL LINER   \n",
       "2       hardware  SPARES FOR OVEN INTERLOCKING SYSTEM OF COB-1,3...   \n",
       "3       hardware      ORIFICE  FLOW METER FOR RAW WATER RISING MAIN   \n",
       "4           none  FIRE FIGHTG ENSEMBLE SET (JACKET,TROUSER,BOOT,...   \n",
       "\n",
       "                                      Content_Parsed  Label_Code  \n",
       "0  procurement  water treatment chemicals  4161 m...           5  \n",
       "1                               spar  rod mill liner           2  \n",
       "2            spar  oven interlock system  cob-135  6           2  \n",
       "3           orifice  flow meter  raw water rise main           2  \n",
       "4  fire fightg ensemble set (jackettrouserbooth/g...           4  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set apart a test set to prove the quality of our models. We'll do Cross Validation in the train set in order to tune the hyperparameters and then test performance on the unseen data of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['Content_Parsed'], \n",
    "                                                    df['Label_Code'], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    195\n",
       "4    126\n",
       "7     84\n",
       "5     67\n",
       "8     35\n",
       "2     32\n",
       "3     29\n",
       "1     19\n",
       "Name: Label_Code, dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    51\n",
       "4    34\n",
       "5    21\n",
       "2    14\n",
       "7    10\n",
       "8     7\n",
       "3     6\n",
       "1     4\n",
       "Name: Label_Code, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't have much observations (only 2.225), we'll choose a test set size of 15% of the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have various options:\n",
    "\n",
    "* Count Vectors as features\n",
    "* TF-IDF Vectors as features\n",
    "* Word Embeddings as features\n",
    "* Text / NLP based features\n",
    "* Topic Models as features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use **TF-IDF Vectors** as features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to define the different parameters:\n",
    "\n",
    "* `ngram_range`: We want to consider both unigrams and bigrams.\n",
    "* `max_df`: When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly higher than the given threshold\n",
    "* `min_df`: When building the vocabulary ignore terms that have a document\n",
    "    frequency strictly lower than the given threshold.\n",
    "* `max_features`: If not None, build a vocabulary that only consider the top\n",
    "    max_features ordered by term frequency across the corpus.\n",
    "\n",
    "See `TfidfVectorizer?` for further detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It needs to be mentioned that we are implicitly scaling our data when representing it as TF-IDF features with the argument `norm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter election\n",
    "ngram_range = (1,3)\n",
    "min_df = 10\n",
    "max_df = 1.\n",
    "max_features = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have chosen these values as a first approximation. Since the models that we develop later have a very good predictive power, we'll stick to these values. But it has to be mentioned that different combinations could be tried in order to improve even more the accuracy of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(587, 116)\n",
      "(147, 116)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(encoding='utf-8',\n",
    "                        ngram_range=ngram_range,\n",
    "                        stop_words=None,\n",
    "                        lowercase=False,\n",
    "                        max_df=max_df,\n",
    "                        min_df=min_df,\n",
    "                        max_features=max_features,\n",
    "                        norm='l2',\n",
    "                        sublinear_tf=True)\n",
    "                        \n",
    "features_train = tfidf.fit_transform(X_train).toarray()\n",
    "labels_train = y_train\n",
    "print(features_train.shape)\n",
    "\n",
    "features_test = tfidf.transform(X_test).toarray()\n",
    "labels_test = y_test\n",
    "print(features_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=777, k_neighbors=1)\n",
    "features_train, labels_train = smt.fit_sample(features_train, labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_test = tfidf.transform(X_test).toarray()\n",
    "# labels_test = y_test\n",
    "# print(features_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we have fitted and then transformed the training set, but we have **only transformed** the **test set**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Chi squared test in order to see what unigrams and bigrams are most correlated with each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 'electronics' category:\n",
      "  . Most correlated unigrams:\n",
      ". per\n",
      ". year\n",
      ". basis\n",
      ". mine\n",
      ". hire\n",
      "  . Most correlated bigrams:\n",
      ". sail ranchi\n",
      ". supply installation\n",
      "\n",
      "# 'hardware' category:\n",
      "  . Most correlated unigrams:\n",
      ". coal\n",
      ". fa\n",
      ". olfa\n",
      ". mt\n",
      ". chemicals\n",
      "  . Most correlated bigrams:\n",
      ". olfa sale\n",
      ". coal chemicals\n",
      "\n",
      "# 'machine' category:\n",
      "  . Most correlated unigrams:\n",
      ". township\n",
      ". handle\n",
      ". area\n",
      ". house\n",
      ". clean\n",
      "  . Most correlated bigrams:\n",
      ". supply installation\n",
      ". rate contract\n",
      "\n",
      "# 'none' category:\n",
      "  . Most correlated unigrams:\n",
      ". water\n",
      ". sms\n",
      ". procurement\n",
      ". sale\n",
      ". items\n",
      "  . Most correlated bigrams:\n",
      ". sail ranchi\n",
      ". supply installation\n",
      "\n",
      "# 'raw_materials' category:\n",
      "  . Most correlated unigrams:\n",
      ". 200520\n",
      ". dt\n",
      ". enable\n",
      ". scrap\n",
      ". waste\n",
      "  . Most correlated bigrams:\n",
      ". division sail\n",
      ". work division\n",
      "\n",
      "# 'skilled_manpower' category:\n",
      "  . Most correlated unigrams:\n",
      ". ote\n",
      ". coke\n",
      ". idle\n",
      ". assets\n",
      ". monitor\n",
      "  . Most correlated bigrams:\n",
      ". supply installation\n",
      ". idle assets\n",
      "\n",
      "# 'unskilled_manpower' category:\n",
      "  . Most correlated unigrams:\n",
      ". supply\n",
      ". installation\n",
      ". commission\n",
      ". machine\n",
      ". conveyor\n",
      "  . Most correlated bigrams:\n",
      ". sail ranchi\n",
      ". supply installation\n",
      "\n",
      "# 'vehicle/equipment_hiring' category:\n",
      "  . Most correlated unigrams:\n",
      ". include\n",
      ". mechanical\n",
      ". electrical\n",
      ". maintenance\n",
      ". repair\n",
      "  . Most correlated bigrams:\n",
      ". sms ii\n",
      ". rate contract\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "import numpy as np\n",
    "\n",
    "for Product, category_id in sorted(category_codes.items()):\n",
    "    features_chi2 = chi2(features_train, labels_train == category_id)\n",
    "    indices = np.argsort(features_chi2[0])\n",
    "    feature_names = np.array(tfidf.get_feature_names())[indices]\n",
    "    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n",
    "    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n",
    "#     trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n",
    "    print(\"# '{}' category:\".format(Product))\n",
    "    print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-5:])))\n",
    "    print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-2:])))\n",
    "#     print(\"  . Most correlated trigrams:\\n. {}\".format('\\n. '.join(trigrams[3:])))\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the unigrams correspond well to their category. However, bigrams do not. If we get the bigrams in our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bokaro steel',\n",
       " 'sru ifico',\n",
       " 'steel plant',\n",
       " 'job contract',\n",
       " 'sail bokaro',\n",
       " 'ore mine',\n",
       " 'iron ore',\n",
       " 'contract ote',\n",
       " 'work division',\n",
       " 'division sail',\n",
       " 'various job',\n",
       " 'dt 250320',\n",
       " 'olfa sale',\n",
       " 'dt 200520',\n",
       " 'idle assets',\n",
       " 'coal chemicals',\n",
       " 'sail ranchi',\n",
       " 'supply installation',\n",
       " 'sms ii',\n",
       " 'rate contract']"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see there is only one. This means the unigrams have more correlation with the category than the bigrams, and since we're restricting the number of features to the most representative 300, only a few bigrams are being considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the files needed in the next steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train\n",
    "with open('Pickles/X_train.pickle', 'wb') as output:\n",
    "    pickle.dump(X_train, output)\n",
    "    \n",
    "# X_test    \n",
    "with open('Pickles/X_test.pickle', 'wb') as output:\n",
    "    pickle.dump(X_test, output)\n",
    "    \n",
    "# y_train\n",
    "with open('Pickles/y_train.pickle', 'wb') as output:\n",
    "    pickle.dump(y_train, output)\n",
    "    \n",
    "# y_test\n",
    "with open('Pickles/y_test.pickle', 'wb') as output:\n",
    "    pickle.dump(y_test, output)\n",
    "    \n",
    "# df\n",
    "with open('Pickles/df.pickle', 'wb') as output:\n",
    "    pickle.dump(df, output)\n",
    "    \n",
    "# features_train\n",
    "with open('Pickles/features_train.pickle', 'wb') as output:\n",
    "    pickle.dump(features_train, output)\n",
    "\n",
    "# labels_train\n",
    "with open('Pickles/labels_train.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_train, output)\n",
    "\n",
    "# features_test\n",
    "with open('Pickles/features_test.pickle', 'wb') as output:\n",
    "    pickle.dump(features_test, output)\n",
    "\n",
    "# labels_test\n",
    "with open('Pickles/labels_test.pickle', 'wb') as output:\n",
    "    pickle.dump(labels_test, output)\n",
    "    \n",
    "# TF-IDF object\n",
    "with open('Pickles/tfidf.pickle', 'wb') as output:\n",
    "    pickle.dump(tfidf, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
